{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Datathon Decision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pt:\n",
        "Notebook usada para desenvolvimento do sistema de recomenda√ß√£o de vagas com base em curr√≠culos proposto na √∫ltimo Tech Challenge do curso de p√≥s-gradua√ß√£o da FIAP em An√°lise de Dados.\n",
        "\n",
        "En:\n",
        "Notebook used for the development of a job possition recomendation system based on CV proposed in the last Tech Challenge of the FIAP postgraduate course in Data Analytics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importanto libs (Importing Libs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWvq-kBkcfx6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udRUwmMqUvo5"
      },
      "source": [
        "## Step 1: Carregando os dados (Loading the Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dElYuSZTUlRB"
      },
      "outputs": [],
      "source": [
        "# Loading JSON files\n",
        "# Carregando arquivos JSON\n",
        "\n",
        "# The JSON files were taken from Decision DB, but first they anonymized the data.\n",
        "# Os arquivos JSON foram pegos da base de dados da Decision, por√©m antes eles deixaram os dados an√¥nimos.\n",
        "\n",
        "with open(r'applicants.json', encoding='utf-8') as f:\n",
        "    applicants = json.load(f)\n",
        "\n",
        "with open(r'vagas.json', encoding='utf-8') as f:\n",
        "    jobs = json.load(f)\n",
        "\n",
        "with open(\"prospects.json\", encoding=\"utf-8\") as f:\n",
        "    prospects = json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìÑ Detalhes dos Dados | Data Details\n",
        "\n",
        "- Os dados est√£o no formato **JSON**.  \n",
        "  _The data is in **JSON** format._\n",
        "  \n",
        "- Todas as informa√ß√µes sens√≠veis (de clientes, candidatos e analistas) foram **anonimizadas**, utilizando nomes, n√∫meros de telefone e e-mails aleat√≥rios.  \n",
        "  _All sensitive information (related to clients, candidates, and analysts) has been **anonymized** using random names, phone numbers, and email addresses._\n",
        "\n",
        "---\n",
        "\n",
        "üìÅ Sobre os Arquivos | About the Files\n",
        "\n",
        "`vagas.json`\n",
        "- Chaveado pelo **c√≥digo da vaga**.  \n",
        "  _Keyed by the **job opening code**._\n",
        "  \n",
        "- Cont√©m informa√ß√µes da vaga no **ATS** (sistema de rastreamento de candidatos), divididas em:\n",
        "  - Informa√ß√µes b√°sicas | _Basic information_\n",
        "  - Perfil da vaga | _Job profile_\n",
        "  - Benef√≠cios | _Benefits_\n",
        "\n",
        "**Campos importantes | Key fields:**\n",
        "- Indica√ß√£o se √© vaga **relacionada a SAP**  \n",
        "  _Indication of whether the job is **SAP-related**_\n",
        "- **Cliente solicitante** | _Requesting client_\n",
        "- **N√≠vel profissional** e **n√≠vel de idiomas** exigidos  \n",
        "  _Required **professional level** and **language proficiency**_\n",
        "- **Atividades principais** e **compet√™ncias t√©cnicas**  \n",
        "  _Key responsibilities and required technical skills_\n",
        "\n",
        "---\n",
        "\n",
        "`prospects.json`\n",
        "- Tamb√©m chaveado pelo **c√≥digo da vaga**.  \n",
        "  _Also keyed by the **job code**._\n",
        "\n",
        "- Cont√©m todas as **prospec√ß√µes** da vaga.  \n",
        "  _Contains all **prospecting entries** related to the job._\n",
        "\n",
        "**Cada entrada inclui | Each entry includes:**\n",
        "- C√≥digo da prospec√ß√£o | _Prospect code_\n",
        "- Nome | _Name_\n",
        "- Coment√°rio | _Comments_\n",
        "- Situa√ß√£o do candidato | _Candidate‚Äôs status_\n",
        "\n",
        "---\n",
        "\n",
        "`applicants.json`\n",
        "- Chaveado pelo **c√≥digo do candidato**.  \n",
        "  _Keyed by the **candidate ID**._\n",
        "\n",
        "- Cont√©m dados:\n",
        "  - B√°sicos | _Basic details_\n",
        "  - Pessoais | _Personal data_\n",
        "  - Profissionais | _Professional background_\n",
        "  - Forma√ß√£o | _Educational background_\n",
        "  - Curr√≠culo completo | _Full r√©sum√©_\n",
        "\n",
        "**Campos importantes | Key fields:**\n",
        "- **N√≠vel acad√™mico**, **ingl√™s** e **espanhol**  \n",
        "  _Academic level, English and Spanish proficiency_\n",
        "- **Conhecimentos t√©cnicos** | _Technical skills_\n",
        "- **√Årea de atua√ß√£o** | _Area of expertise_\n",
        "- **CV completo** | _Full r√©sum√©_\n",
        "\n",
        "---\n",
        "\n",
        "üß™ Exemplo de Utiliza√ß√£o | Example Use Case\n",
        "\n",
        "A vaga **`10976`** (chave no `vagas.json`) possui **25 prospec√ß√µes** (chave `10976` no `prospects.json`), entre as quais o candidato **‚ÄúSr. Thales Freitas‚Äù** (chave `41496` no `applicants.json`) foi **contratado**.  \n",
        "_Job opening **`10976`** (key in `vagas.json`) has **25 prospects** (key `10976` in `prospects.json`), among which candidate **‚ÄúSr. Thales Freitas‚Äù** (key `41496` in `applicants.json`) was **hired**._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bRqLGJPU1s0"
      },
      "source": [
        "## Step 2: Extraindo os textos como habilidades e descri√ß√µes (Extracting Text like Skills & Descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVnczqMkU22v"
      },
      "outputs": [],
      "source": [
        "\n",
        "# From applicants\n",
        "# Dos aplicantes\n",
        "def extract_applicant_skills(applicant):\n",
        "    skills = applicant[\"informacoes_profissionais\"].get(\"conhecimentos_tecnicos\", \"\")\n",
        "    cv = applicant.get(\"cv_pt\", \"\")\n",
        "    return skills + \" \" + cv.lower()  # merge and normalize\n",
        "\n",
        "# From jobs\n",
        "# Das vagas\n",
        "def extract_job_requirements(job):\n",
        "    skills = job[\"perfil_vaga\"].get(\"competencia_tecnicas_e_comportamentais\", \"\")\n",
        "    activities = job[\"perfil_vaga\"].get(\"principais_atividades\", \"\")\n",
        "    return skills.lower() + \" \" + activities.lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbUWe_mtVQ0W"
      },
      "source": [
        "##  Step 3: Criando um dataset de similaridades (Creating Matching Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui0Apgbss87r",
        "outputId": "8b477938-73a7-4ae2-8476-951d0012df60"
      },
      "outputs": [],
      "source": [
        "# Criando fun√ß√£o que remover√° stop words e limpara o texto\n",
        "# Creating function that will remove stop words and clean the text\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text, language='portuguese')\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the ids\n",
        "# Obtendo os ids\n",
        "applicant_ids = list(applicants.keys())\n",
        "job_ids = list(jobs.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaQMuedSs_Uu"
      },
      "outputs": [],
      "source": [
        "# Extracting and preprocessing texts\n",
        "# Extraindo e limpando o texto\n",
        "applicant_texts = [\n",
        "    preprocess(extract_applicant_skills(applicants[aid]))\n",
        "    for aid in applicant_ids\n",
        "]\n",
        "\n",
        "job_texts = [\n",
        "    preprocess(extract_job_requirements(jobs[jid]))\n",
        "    for jid in job_ids\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooWubl29VSnC"
      },
      "outputs": [],
      "source": [
        "# Initializing a TF-IDF vectorizer, which is converting text into numerical form\n",
        "\n",
        "# Inicializando um vetor TF-IDF, que est√° convertendo texto em uma forma num√©rica\n",
        "\n",
        "# Creating a tool that is converting text into numbers based on word importance\n",
        "\n",
        "# Criando uma ferramenta que est√° convertendo texto em n√∫meros com base na import√¢ncia das palavras\n",
        "vectorizer = TfidfVectorizer()  \n",
        "\n",
        "# Fitting the vectorizer on the job descriptions and transforming them into numerical vectors\n",
        "\n",
        "# Ajustando o vetor TF-IDF nas descri√ß√µes de vagas e transformando-as em vetores num√©ricos\n",
        "job_vecs = vectorizer.fit_transform(job_texts)  \n",
        "# The vectorizer is learning from the job descriptions, finding important words and how often they are appearing\n",
        "# Then, it is converting these descriptions into numerical vectors that a machine learning model can understand\n",
        "\n",
        "# O vetor est√° aprendendo com as descri√ß√µes de vagas, encontrando palavras importantes e com que frequ√™ncia elas est√£o aparecendo\n",
        "# Em seguida, est√° convertendo essas descri√ß√µes em vetores num√©ricos que um modelo de aprendizado de m√°quina pode entender\n",
        "\n",
        "# Transforming the applicants' texts into the same numerical format using the same vectorizer\n",
        "\n",
        "# Transformando os textos dos candidatos para o mesmo formato num√©rico usando o mesmo vetor TF-IDF\n",
        "applicant_vecs = vectorizer.transform(applicant_texts) \n",
        "\n",
        "# Now, we are using the same learned vectorizer to convert the applicants' texts into vectors in the same way\n",
        "# This is allowing us to compare the job descriptions and the applicant texts, since both are now in the same numerical form\n",
        "\n",
        "# Agora, estamos usando o mesmo vetor aprendido para converter os textos dos candidatos em vetores da mesma maneira\n",
        "# Isso est√° nos permitindo comparar as descri√ß√µes de vagas e os textos dos candidatos, j√° que ambos est√£o no mesmo formato num√©rico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHqsN0wfwbc7"
      },
      "outputs": [],
      "source": [
        "# Calculating the cosine similarity between the applicant vectors and the job vectors\n",
        "# A fun√ß√£o cosine_similarity est√° calculando a similaridade entre os vetores dos candidatos e os vetores das vagas\n",
        "similarity_matrix = cosine_similarity(applicant_vecs, job_vecs)\n",
        "\n",
        "# O resultado √© uma matriz que mostra o qu√£o semelhantes s√£o os textos dos candidatos em rela√ß√£o √†s vagas\n",
        "# The result is a similarity matrix where each element represents how similar a specific applicant‚Äôs text is to a specific job description. A higher value means a higher similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This was used to save the output from similarity matrix and reuse so we don't spending time running everytime that we need\n",
        "# Isso foi usado para salvar o resultado da matriz de similaridade e reutiliz√°-la, para que n√£o gastemos tempo rodando o c√°lculo toda vez que precisarmos\n",
        "np.save('similarity_matrix', similarity_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDA2KM8FcV2G"
      },
      "source": [
        "## Step 4: Recomendando principais vagas para aplicantes (Recommending Top Jobs for an Applicant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIIFQXiGcUtC"
      },
      "outputs": [],
      "source": [
        "# This function shows job recommendations for a specific applicant based on the similarity between their profile and the jobs\n",
        "# Esta fun√ß√£o mostra a recomenda√ß√£o de vagas para um candidato espec√≠fico baseado na similaridade entre seus perfis e as vagas\n",
        "def show_recommendations_for_applicant(applicant_index, top_n=5):\n",
        "    # Getting the applicant's ID using the index in the list of applicant IDs\n",
        "    # Obtendo o ID do aplinte utilizando o index\n",
        "    applicant_id = applicant_ids[applicant_index]\n",
        "\n",
        "    # Getting the applicant's information from the applicants dictionary\n",
        "    # Obetendo as informa√ß√µes do candidato do arquivo JSON\n",
        "    applicant = applicants.get(applicant_id, {})  \n",
        "\n",
        "    # Extracting important details from the applicant's profile\n",
        "    # Obtendo ingorma√ß√µes detalahas do perfil do aplicante\n",
        "    name = applicant.get(\"infos_basicas\", {}).get(\"nome\", \"N/A\")\n",
        "    area = applicant.get(\"informacoes_profissionais\", {}).get(\"area_atuacao\", \"N/A\")\n",
        "    skills = applicant.get(\"informacoes_profissionais\", {}).get(\"conhecimentos_tecnicos\", \"N/A\")\n",
        "    academic = applicant.get(\"formacao_e_idiomas\", {}).get(\"nivel_academico\", \"N/A\")\n",
        "    english = applicant.get(\"formacao_e_idiomas\", {}).get(\"nivel_ingles\", \"N/A\")\n",
        "    spanish = applicant.get(\"formacao_e_idiomas\", {}).get(\"nivel_espanhol\", \"N/A\")\n",
        "    cv_excerpt = applicant.get(\"cv_pt\", \"\").strip().replace(\"\\n\", \" \")[:300] + \"...\"\n",
        "\n",
        "    # Displaying the applicant's basic information\n",
        "    # Mostrando as informa√ß√µes do aplicante\n",
        "    print(f\"\\n=== üßë Applicant: {name} (ID: {applicant_id}) ===\")\n",
        "    print(f\"√Årea de Atua√ß√£o: {area}\")\n",
        "    print(f\"Conhecimentos T√©cnicos: {skills}\")\n",
        "    print(f\"Forma√ß√£o: {academic} | Ingl√™s: {english} | Espanhol: {spanish}\")\n",
        "    print(f\"üìÑ CV (resumo): {cv_excerpt}\\n\")\n",
        "    print('\\n--------------------------------------------------------------------\\n')\n",
        "\n",
        "    # Getting the similarity scores between this applicant and all the jobs\n",
        "    # Obtendo o valor de similaridade entre os aplicantes e todas as vagas\n",
        "    sim_scores = similarity_matrix[applicant_index]\n",
        "    top_indices = sim_scores.argsort()[::-1][:top_n] \n",
        "\n",
        "    # Looping through the top job recommendations based on similarity score to get job details\n",
        "    # Iterando sobre as recomenda√ß√µes para obter informa√ß√µes das vagas\n",
        "    for j in top_indices:\n",
        "        job_id = job_ids[j]\n",
        "        job = jobs.get(job_id, {})\n",
        "        job_title = job.get(\"informacoes_basicas\", {}).get(\"titulo_vaga\", \"N/A\")\n",
        "        job_area = job.get(\"perfil_vaga\", {}).get(\"areas_atuacao\", \"N/A\")\n",
        "        job_skills = job.get(\"perfil_vaga\", {}).get(\"competencia_tecnicas_e_comportamentais\", \"N/A\")\n",
        "        job_activities = job.get(\"perfil_vaga\", {}).get(\"principais_atividades\", \"N/A\")\n",
        "\n",
        "        # Displaying the job recommendation and details\n",
        "        # Mostrando os detalhes da recomenda√ß√£o\n",
        "        print(f\"üîπ Job Recommendation: {job_title} (ID: {job_id})\")\n",
        "        print(f\"   Similarity Score: {sim_scores[j]:.2f}\")  # Display the similarity score for the job\n",
        "        print(f\"   √Årea: {job_area}\")\n",
        "        print(f\"   üîß Compet√™ncias: {job_skills[:250]}...\")  # Show a snippet of the job's skills required\n",
        "        print(f\"   üìã Atividades: {job_activities[:250]}...\\n\")  # Show a snippet of the job's activities\n",
        "        print('--------------------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB3JZFE-cBg5"
      },
      "outputs": [],
      "source": [
        "show_recommendations_for_applicant(1, top_n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En:\n",
        "Now that we have the similarity and the recommendations, we will use Logistic Regression to predict the probability that one applicant will be hired based on the similarity score.\n",
        "\n",
        "Pt:\n",
        "Agora que temos a similaridade e as recomenda√ß√µes, usaremos a Regress√£o Log√≠stica para prever a probabilidade de um candidato ser contratado com base na pontua√ß√£o de similaridade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a dictionary with Job ID, applicat ID and status.\n",
        "# We are using the file prospects to get the information of which job was filled with which applicant.\n",
        "\n",
        "# Criando um dicion√°rio com ID da Vaga, ID do Candidato e Status.\n",
        "# Estamos usando o arquivo de prospects para obter as informa√ß√µes sobre qual vaga foi preenchida com qual candidato.\n",
        "\n",
        "job_applicant_status = defaultdict(dict)\n",
        "\n",
        "# job_id ‚Üí applicant_id ‚Üí status\n",
        "for job_id, job_data in prospects.items():\n",
        "    for prospect in job_data.get(\"prospects\", []):\n",
        "        applicant_id = prospect[\"codigo\"]\n",
        "        status = prospect.get(\"situacao_candidado\", \"\")\n",
        "        job_applicant_status[job_id][applicant_id] = status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardizing the status for ML prediction\n",
        "# Padronizando os status para predi√ß√£o do modelo.\n",
        "\n",
        "def label_from_status(status):\n",
        "    status = status.lower()\n",
        "    if \"contratado\" in status or \"fechado\" in status or \"encaminhado\" in status:\n",
        "        return 1  # Match\n",
        "    else:\n",
        "        return 0  # no Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a merge DF with similarity data and now the status data.\n",
        "# Crriando um dataframe consolidado com os valores de similaridade e os status obtidos agora.\n",
        "\n",
        "records = []\n",
        "TOP_N = 10\n",
        "\n",
        "for i, applicant_id in enumerate(applicant_ids):\n",
        "    sim_scores = similarity_matrix[i]\n",
        "    top_indices = sim_scores.argsort()[::-1][:TOP_N]\n",
        "\n",
        "    for j in top_indices:\n",
        "        print(i, len(applicant_ids))\n",
        "        job_id = job_ids[j]\n",
        "        sim = sim_scores[j]\n",
        "\n",
        "        status = job_applicant_status.get(job_id, {}).get(applicant_id, \"\")\n",
        "        label = label_from_status(status)\n",
        "\n",
        "        records.append({\n",
        "            \"applicant_id\": applicant_id,\n",
        "            \"job_id\": job_id,\n",
        "            \"similarity_score\": sim,\n",
        "            \"status\": status,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Counting all the possibility of status before the standadization.\n",
        "# Contando todas as pssibilidades de status antes da padroniza√ß√£o.\n",
        "\n",
        "df['status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the Df so we save time if running the code again is needed.\n",
        "# Salvando o dataframe caso precisemos dele novamente.\n",
        "\n",
        "df.to_pickle('labeled_df')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running the Logistic Regression model\n",
        "# Rodando o modelo de Regress√£o Log√≠stica\n",
        "\n",
        "df[\"binary_label\"] = df[\"label\"]\n",
        "\n",
        "X = df[[\"similarity_score\"]]\n",
        "y = df[\"binary_label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running the XGB model\n",
        "# Rodando o modelo de XGB\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(\n",
        "    scale_pos_weight=80000 / 139,  # imbalance ratio\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Because of unbalanced data, we will need to balance our DF with similar proportion of 0 and 1.\n",
        "# Por conta dos dados desbalanceados, teremos que balancear nosso dataframe como propor√ß√£o similar de 0 e 1.\n",
        "from sklearn.utils import resample\n",
        "\n",
        "df_majority = df[df.label == 0]\n",
        "df_minority = df[df.label == 1]\n",
        "\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,\n",
        "                                   n_samples=len(df_minority) * 3,\n",
        "                                   random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_majority_downsampled, df_minority])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running once again the Logistic Regression model after the resample\n",
        "# Rodando o modelo de Regress√£o log√≠stica mais uma vez depois do resample\n",
        "\n",
        "df_balanced[\"binary_label\"] = df_balanced[\"label\"]\n",
        "\n",
        "X = df_balanced[[\"similarity_score\"]]\n",
        "y = df_balanced[\"binary_label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running once again the XGB model after the resample\n",
        "# Rodando o modelo de XGB mais uma vez depois do resample\n",
        "\n",
        "model = XGBClassifier(\n",
        "    scale_pos_weight=80000 / 139,  # imbalance ratio\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recreating the job recommendation function but now with the hire probability\n",
        "# Recriando a fun√ß√£o de recomenda√ß√£o de vagas, por√©m agora com a probilidade de contrata√ß√£o\n",
        "def show_recommendations_for_applicant(applicant_index, top_n=5, model=None):\n",
        "    applicant_id = applicant_ids[applicant_index]\n",
        "    applicant = applicants.get(applicant_id, {})\n",
        "\n",
        "    # Extract applicant details\n",
        "    name = applicant.get(\"infos_basicas\", {}).get(\"nome\", \"N/A\")\n",
        "    area = applicant.get(\"informacoes_profissionais\", {}).get(\"area_atuacao\", \"N/A\")\n",
        "    skills = applicant.get(\"informacoes_profissionais\", {}).get(\"conhecimentos_tecnicos\", \"N/A\")\n",
        "    academic = applicant.get(\"formacao_e_idiomas\", {}).get(\"nivel_academico\", \"N/A\")\n",
        "    english = applicant.get(\"formacao_e_idiomas\", {}).get(\"nivel_ingles\", \"N/A\")\n",
        "    spanish = applicant.get(\"formacao_e_idiomas\", {}).get(\"nivel_espanhol\", \"N/A\")\n",
        "    cv_excerpt = applicant.get(\"cv_pt\", \"\").strip().replace(\"\\n\", \" \")[:300] + \"...\"\n",
        "\n",
        "    # Show applicant info once\n",
        "    print(f\"\\n=== üßë Applicant: {name} (ID: {applicant_id}) ===\")\n",
        "    print(f\"√Årea de Atua√ß√£o: {area}\")\n",
        "    print(f\"Conhecimentos T√©cnicos: {skills}\")\n",
        "    print(f\"Forma√ß√£o: {academic} | Ingl√™s: {english} | Espanhol: {spanish}\")\n",
        "    print(f\"üìÑ CV (resumo): {cv_excerpt}\\n\")\n",
        "    print('\\n--------------------------------------------------------------------\\n')\n",
        "\n",
        "    # Job recommendations\n",
        "    sim_scores = similarity_matrix[applicant_index]\n",
        "    top_indices = sim_scores.argsort()[::-1][:top_n]\n",
        "\n",
        "    for j in top_indices:\n",
        "        job_id = job_ids[j]\n",
        "        job = jobs.get(job_id, {})\n",
        "        job_title = job.get(\"informacoes_basicas\", {}).get(\"titulo_vaga\", \"N/A\")\n",
        "        job_area = job.get(\"perfil_vaga\", {}).get(\"areas_atuacao\", \"N/A\")\n",
        "        job_skills = job.get(\"perfil_vaga\", {}).get(\"competencia_tecnicas_e_comportamentais\", \"N/A\")\n",
        "        job_activities = job.get(\"perfil_vaga\", {}).get(\"principais_atividades\", \"N/A\")\n",
        "\n",
        "        # Get similarity score\n",
        "        sim_score = sim_scores[j]\n",
        "\n",
        "        # Use model to predict hire probability\n",
        "        hire_prob = None\n",
        "        if model is not None:\n",
        "            # Model expects 2D array of features\n",
        "            hire_prob = model.predict_proba([[sim_score]])[0][1]  # probability of class 1 (hired)\n",
        "\n",
        "        print(f\"üîπ Job Recommendation: {job_title} (ID: {job_id})\")\n",
        "        print(f\"   Similarity Score: {sim_score:.2f}\")\n",
        "        if hire_prob is not None:\n",
        "            print(f\"   ü§ñ Predicted Hire Probability: {hire_prob:.2%}\")\n",
        "        print(f\"   √Årea: {job_area}\")\n",
        "        print(f\"   üîß Compet√™ncias: {job_skills[:250]}...\")\n",
        "        print(f\"   üìã Atividades: {job_activities[:250]}...\\n\")\n",
        "        print('--------------------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_recommendations_for_applicant(applicant_index=300, top_n=5, model=model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A biblioteca `sentence-transformers` fornece uma maneira poderosa de transformar texto em **embeddings** (representa√ß√µes vetoriais densas) que capturam o significado sem√¢ntico do texto. Isso √© diferente do m√©todo tradicional **TF-IDF**, que se baseia na frequ√™ncia das palavras e n√£o entende o significado subjacente das palavras e frases.\n",
        "\n",
        "This code uses the `sentence-transformers` library to convert text into **embeddings** (dense vector representations) that capture the semantic meaning of the text. This is different from the traditional **TF-IDF** method, which relies on word frequencies and does not understand the underlying meaning of words and sentences.\n",
        "\n",
        "Neste c√≥digo, estamos utilizando o modelo `paraphrase-multilingual-MiniLM-L12-v2`, que √© um modelo **multil√≠ngue** treinado para gerar embeddings de alta qualidade para diferentes idiomas.\n",
        "\n",
        "In this code, we are using the model `paraphrase-multilingual-MiniLM-L12-v2`, which is a **multilingual** model trained to generate high-quality embeddings for different languages.\n",
        "\n",
        "O modelo √© carregado da seguinte forma:\n",
        "\n",
        "The model is loaded as follows:\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # good multilingual model\n",
        "```\n",
        "\n",
        "Em seguida, usamos o m√©todo `encode()` para converter os textos dos **candidatos** e **empregos** em embeddings:\n",
        "\n",
        "Next, we use the `encode()` method to convert the **applicant** and **job** texts into embeddings:\n",
        "\n",
        "```python\n",
        "applicant_embeddings = model.encode(applicant_texts, show_progress_bar=True)\n",
        "job_embeddings = model.encode(job_texts, show_progress_bar=True)\n",
        "```\n",
        "\n",
        "Os embeddings gerados s√£o **vetores num√©ricos** que representam o significado do texto de uma maneira compacta e sem√¢ntica. Isso nos permite comparar a semelhan√ßa entre os textos de forma eficaz.\n",
        "\n",
        "The generated embeddings are **numeric vectors** that represent the meaning of the text in a compact and semantic way. This allows us to compare the similarity between texts effectively.\n",
        "\n",
        "Para calcular a **similaridade** entre os textos dos candidatos e as vagas, usamos o **cosine similarity**:\n",
        "\n",
        "To calculate the **similarity** between the applicant and job texts, we use **cosine similarity**:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix = cosine_similarity(applicant_embeddings, job_embeddings)\n",
        "```\n",
        "\n",
        "O `cosine similarity` nos d√° um valor entre -1 e 1 que representa a **semelhan√ßa** entre os vetores, com valores mais pr√≥ximos de 1 indicando maior semelhan√ßa.\n",
        "\n",
        "Cosine similarity gives us a value between -1 and 1 that represents the **similarity** between the vectors, with values closer to 1 indicating higher similarity.\n",
        "\n",
        "---\n",
        "\n",
        "**Compara√ß√£o entre Embeddings e TF-IDF**\n",
        "\n",
        "**TF-IDF (Term Frequency-Inverse Document Frequency)** √© uma abordagem tradicional para representar texto numericamente. Ele se baseia na **frequ√™ncia das palavras** em um documento e a import√¢ncia das palavras em todo o conjunto de dados. O TF-IDF cria **vetores esparsos**, onde cada palavra tem uma posi√ß√£o pr√≥pria no vetor, e o valor dessa posi√ß√£o √© a frequ√™ncia ponderada da palavra. \n",
        "\n",
        "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a traditional approach to numerically represent text. It relies on the **frequency of words** in a document and the importance of words across the dataset. TF-IDF creates **sparse vectors**, where each word has its own position in the vector, and the value of that position is the weighted frequency of the word.\n",
        "\n",
        "Por√©m, TF-IDF **n√£o captura o significado sem√¢ntico** das palavras. Ele trata palavras diferentes como se fossem completamente diferentes, mesmo que tenham significados semelhantes. Por exemplo:\n",
        "\n",
        "However, TF-IDF **does not capture the semantic meaning** of words. It treats different words as completely different, even if they have similar meanings. For example:\n",
        "\n",
        "- \"Eu amo programar.\" e \"Eu gosto de programar.\"\n",
        "- \"I love programming.\" and \"I enjoy programming.\"\n",
        "\n",
        "No TF-IDF, essas duas frases podem ser representadas de forma bastante diferente, j√° que \"amo\" e \"gosto\" s√£o palavras distintas.\n",
        "\n",
        "In TF-IDF, these two sentences might be represented quite differently because \"love\" and \"enjoy\" are distinct words.\n",
        "\n",
        "Em contrapartida, os **Embeddings** s√£o vetores densos que representam o **significado** sem√¢ntico das palavras ou frases. Modelos como o **Word2Vec**, **GloVe** e **Sentence Transformers** convertem palavras ou senten√ßas em vetores, onde palavras com significados semelhantes s√£o representadas por vetores **pr√≥ximos** no espa√ßo vetorial. Por exemplo:\n",
        "\n",
        "On the other hand, **Embeddings** are dense vectors that represent the **semantic meaning** of words or sentences. Models like **Word2Vec**, **GloVe**, and **Sentence Transformers** convert words or sentences into vectors, where words with similar meanings are represented by **close** vectors in the vector space. For example:\n",
        "\n",
        "- \"Amo\" e \"gosto\" teriam vetores pr√≥ximos, j√° que ambas as palavras t√™m significados semelhantes.\n",
        "- \"Love\" and \"enjoy\" would have similar vectors, as both words have similar meanings.\n",
        "\n",
        "**Vantagens do uso de Embeddings:**\n",
        "\n",
        "**Advantages of Using Embeddings:**\n",
        "\n",
        "1. **Entendimento Sem√¢ntico (Significado Contextual)**: Embeddings conseguem **capturar o significado** das palavras e frases, o que √© importante para tarefas como compara√ß√£o de textos e busca sem√¢ntica.\n",
        "   - **Semantic Understanding (Contextual Meaning)**: Embeddings can **capture the meaning** of words and sentences, which is crucial for tasks like text comparison and semantic search.\n",
        "\n",
        "2. **Sin√¥nimos e Parafraseamento**: Embeddings conseguem entender que palavras com significados semelhantes (sin√¥nimos) ou frases com significados iguais (par√°frases) s√£o semelhantes.\n",
        "   - **Synonyms and Paraphrasing**: Embeddings can understand that words with similar meanings (synonyms) or sentences with the same meaning (paraphrases) are similar.\n",
        "\n",
        "3. **Vetores Densos e Compactos**: Diferente do TF-IDF, que cria vetores esparsos (com muitas posi√ß√µes com valor 0), os embeddings criam vetores **densos**, com informa√ß√µes ricas e compactas.\n",
        "   - **Dense and Compact Vectors**: Unlike TF-IDF, which creates sparse vectors (with many zero-value positions), embeddings create **dense** vectors with rich and compact information.\n",
        "\n",
        "4. **Melhor Generaliza√ß√£o**: Embeddings podem ser **transferidos** entre diferentes conjuntos de dados, aproveitando o treinamento pr√©vio de modelos em grandes corpora de texto, enquanto o TF-IDF √© limitado ao conjunto de dados em que √© treinado.\n",
        "   - **Better Generalization**: Embeddings can be **transferred** between different datasets, leveraging the prior training of models on large text corpora, while TF-IDF is limited to the dataset it was trained on.\n",
        "\n",
        "**Quando Usar TF-IDF**:\n",
        "- Quando voc√™ precisa de uma solu√ß√£o simples e direta e n√£o se importa com o significado sem√¢ntico do texto.\n",
        "- Para tarefas como **classifica√ß√£o de documentos** em que a frequ√™ncia das palavras √© mais importante que o significado.\n",
        "- Quando o conjunto de dados √© **menor** e n√£o exige uma compreens√£o profunda do contexto.\n",
        "\n",
        "**When to Use TF-IDF**:\n",
        "- When you need a simple and direct solution and don't care about the semantic meaning of the text.\n",
        "- For tasks like **document classification** where word frequency is more important than meaning.\n",
        "- When the dataset is **smaller** and doesn't require a deep understanding of context.\n",
        "\n",
        "**Quando Usar Embeddings**:\n",
        "- Quando voc√™ precisa capturar o **significado sem√¢ntico** do texto, como em tarefas de **similaridade de texto**, **respostas a perguntas**, ou **busca sem√¢ntica**.\n",
        "- Quando o seu conjunto de dados √© **grande** e cont√©m variabilidade lingu√≠stica (sin√¥nimos, parafraseamento, etc.).\n",
        "- Quando voc√™ precisa lidar com **dados multil√≠ngues** e deseja uma representa√ß√£o consistente do significado em diferentes idiomas.\n",
        "\n",
        "**When to Use Embeddings**:\n",
        "- When you need to capture the **semantic meaning** of the text, such as in **text similarity**, **question answering**, or **semantic search** tasks.\n",
        "- When your dataset is **large** and contains linguistic variability (synonyms, paraphrasing, etc.).\n",
        "- When you need to handle **multilingual data** and want a consistent representation of meaning across languages.\n",
        "\n",
        "---\n",
        "\n",
        "**TF-IDF** √© uma boa op√ß√£o para tarefas simples, enquanto os **Embeddings** s√£o mais poderosos para entender o contexto e a sem√¢ntica do texto em tarefas mais complexas, como a recomenda√ß√£o de empregos com base no significado de curr√≠culos e descri√ß√µes de vagas.\n",
        "\n",
        "**TF-IDF** is a good option for simple tasks, while **Embeddings** are more powerful for understanding the context and semantics of text in more complex tasks, such as recommending jobs based on the meaning of resumes and job descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # good multilingual model\n",
        "\n",
        "# Convert preprocessed text\n",
        "applicant_embeddings = model.encode(applicant_texts, show_progress_bar=True)\n",
        "job_embeddings = model.encode(job_texts, show_progress_bar=True)\n",
        "\n",
        "# Similarity matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity_matrix = cosine_similarity(applicant_embeddings, job_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the new similarity matrix with embeddings in case we need it again\n",
        "# Salvando a nova matriz de similariedade feita com embedding caso precisemos dela novamente.\n",
        "np.save('similarity_matrix_emb', similarity_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_recommendations_for_applicant(1, top_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating once again the combined DF with similarity scores and status from prospects\n",
        "# Criando novamente o DF com informa√ß√µes de similaride e status do arquivo prospect\n",
        "records = []\n",
        "TOP_N = 10\n",
        "\n",
        "for i, applicant_id in enumerate(applicant_ids):\n",
        "    sim_scores = similarity_matrix[i]\n",
        "    top_indices = sim_scores.argsort()[::-1][:TOP_N]\n",
        "\n",
        "    for j in top_indices:\n",
        "        print(i, len(applicant_ids))\n",
        "        job_id = job_ids[j]\n",
        "        sim = sim_scores[j]\n",
        "\n",
        "        status = job_applicant_status.get(job_id, {}).get(applicant_id, \"\")\n",
        "        label = label_from_status(status)\n",
        "\n",
        "        records.append({\n",
        "            \"applicant_id\": applicant_id,\n",
        "            \"job_id\": job_id,\n",
        "            \"similarity_score\": sim,\n",
        "            \"status\": status,\n",
        "            \"label\": label\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the new DF with embeddings in case we need it again\n",
        "# Salvando o novo dataframe feito com embedding caso precisemos dele novamente.\n",
        "df.to_pickle('labeled_df_emb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Balancing the new df\n",
        "# Balanceando o novo dataframe\n",
        "\n",
        "df_majority = df[df.label == 0]\n",
        "df_minority = df[df.label == 1]\n",
        "\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,\n",
        "                                   n_samples=len(df_minority) * 3,\n",
        "                                   random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_majority_downsampled, df_minority])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running the Logistic Regression model again with the embedding's data\n",
        "# Rodando o modelo de Regress√£o Log√≠stica novamente com os novos dados do embedding\n",
        "\n",
        "df_balanced[\"binary_label\"] = df_balanced[\"label\"]\n",
        "\n",
        "X = df_balanced[[\"similarity_score\"]]\n",
        "y = df_balanced[\"binary_label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running the XGB model again with the embedding's data\n",
        "# Rodando o modelo de XGB novamente com os novos dados do embedding\n",
        "\n",
        "model = XGBClassifier(\n",
        "    scale_pos_weight=80000 / 139,  # imbalance ratio\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_recommendations_for_applicant(applicant_index=300, top_n=5, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
